{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFestimatorAPI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gr3ybr0w/cookbook/blob/master/Machine_Learning/Neural_Networks/TensorFlow/TFestimatorAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hIW43OZuMXFe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "import pickle\n",
        "import psutil\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_columns = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bc4XT3VzMnlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# config\n",
        "KILL_TENSORBOARD = True\n",
        "FRESH_START = True\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 40\n",
        "HIDDEN_UNITS = [1028, 1028, 1028]\n",
        "LEARNING_RATE = 0.0001\n",
        "DROPOUT = 0.25\n",
        "VERSION = tf.__version__\n",
        "CWD = os.getcwd()\n",
        "PARENT_DIR = os.path.split(CWD)[0]\n",
        "DATETIME = datetime.datetime.utcnow()\n",
        "DATA_DIR = os.path.join(PARENT_DIR, 'data')\n",
        "MODEL_DIR = os.path.join(PARENT_DIR, 'models')\n",
        "TARGET_COLUMN = 'sumUKQuantity'\n",
        "MODEL_TYPE_DIR = os.path.join(MODEL_DIR, 'UKpreds')\n",
        "MODEL_ZOO = os.path.join(PARENT_DIR, 'model_zoo')\n",
        "\n",
        "print(\"Kill tensorboard: {}\".format(KILL_TENSORBOARD))\n",
        "print(\"Starting a fresh: {}\".format(FRESH_START))\n",
        "print(\"Running for {} epochs\".format(EPOCHS))\n",
        "print(\"Batch size: {}\".format(BATCH_SIZE))\n",
        "print(\"Hidden Units: {}\".format(str(HIDDEN_UNITS)))\n",
        "print(\"Learning rate: {}\".format(LEARNING_RATE))\n",
        "print(\"Dropout: {}\".format(DROPOUT))\n",
        "print(\"Using TensorFlow version: {}\".format(VERSION))\n",
        "print(\"Current working directory: {}\".format(CWD))\n",
        "print(\"Parent director: {}\".format(PARENT_DIR))\n",
        "print(\"Date time: {}\".format(DATETIME))\n",
        "print(\"Data directory: {}\".format(DATA_DIR))\n",
        "print(\"Model directory: {}\".format(MODEL_DIR))\n",
        "print(\"Target column {}\".format(TARGET_COLUMN))\n",
        "print(\"Model type directory: {}\".format(MODEL_TYPE_DIR))\n",
        "print(\"Model zoo directory: {}\".format(MODEL_ZOO))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KsLRwxtMnuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def killTensorboard():\n",
        "    PROCNAME = \"tensorboard.exe\"\n",
        "    for proc in psutil.process_iter():\n",
        "        # check whether the process name matches\n",
        "        if proc.name() == PROCNAME:\n",
        "            proc.kill()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oLfla3WGMnw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if KILL_TENSORBOARD:\n",
        "    killTensorboard()\n",
        "\n",
        "if FRESH_START:\n",
        "    shutil.rmtree(MODEL_TYPE_DIR, ignore_errors = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESFWdNP0MnzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fileObject = open(os.path.join(DATA_DIR, 'feateng_parameters'),'rb')  \n",
        "# load the object from the file into var b\n",
        "feature_params = pickle.load(fileObject)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYDLWJYLMn18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_vocab_file(col_name):\n",
        "    filename = col_name + 'cats.txt'\n",
        "    return os.path.join(DATA_DIR, 'catfiles', filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RL-8VoHcMn4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_DNNfeature_cols():\n",
        "    # Continuous columns\n",
        "    UnitCostPrice = tf.feature_column.numeric_column(key='UnitCostPrice')\n",
        "#     Year = tf.feature_column.numeric_column(key='Year')\n",
        "    RRP_GBP = tf.feature_column.numeric_column(key='RRP_GBP')\n",
        "    ExVatSalesValue = tf.feature_column.numeric_column(key='ExVatSalesValue')\n",
        "    \n",
        "    # categorical columns\n",
        "    Week = tf.feature_column.categorical_column_with_vocabulary_file(key='Week', vocabulary_file=get_vocab_file('Week'),vocabulary_size=None)\n",
        "    Quarter = tf.feature_column.categorical_column_with_vocabulary_list(key='Quarter', vocabulary_list=['1','2','3','4'])\n",
        "    UKSize = tf.feature_column.categorical_column_with_vocabulary_file(key='UKSize', vocabulary_file=get_vocab_file('UKSize'), vocabulary_size=None)\n",
        "    BaseColour = tf.feature_column.categorical_column_with_vocabulary_file(key='BaseColour', vocabulary_file=get_vocab_file('BaseColour'), vocabulary_size=None)\n",
        "    ProductBrand = tf.feature_column.categorical_column_with_vocabulary_file(key='ProductBrand', vocabulary_file=get_vocab_file('ProductBrand'), vocabulary_size=None)\n",
        "    Department = tf.feature_column.categorical_column_with_vocabulary_file(key='Department', vocabulary_file=get_vocab_file('Department'), vocabulary_size=None)\n",
        "    ProductType = tf.feature_column.categorical_column_with_vocabulary_file(key='ProductType', vocabulary_file=get_vocab_file('ProductType'), vocabulary_size=None)\n",
        "    ProductSubType = tf.feature_column.categorical_column_with_vocabulary_file(key='ProductSubType', vocabulary_file=get_vocab_file('ProductSubType'), vocabulary_size=None)\n",
        "    Silo = tf.feature_column.categorical_column_with_vocabulary_file(key='Silo', vocabulary_file=get_vocab_file('Silo'), vocabulary_size=None)\n",
        "    SubSilo = tf.feature_column.categorical_column_with_vocabulary_file(key='SubSilo', vocabulary_file=get_vocab_file('SubSilo'), vocabulary_size=None)\n",
        "    Level = tf.feature_column.categorical_column_with_vocabulary_file(key='Level', vocabulary_file=get_vocab_file('Level'), vocabulary_size=None)\n",
        "    Sport = tf.feature_column.categorical_column_with_vocabulary_file(key='Sport', vocabulary_file=get_vocab_file('Sport'), vocabulary_size=None)\n",
        "    Supplier = tf.feature_column.categorical_column_with_vocabulary_file(key='Supplier', vocabulary_file=get_vocab_file('Supplier'), vocabulary_size=None)\n",
        "    \n",
        "\n",
        "    feature_columns = [\n",
        "        UnitCostPrice,\n",
        "        RRP_GBP,\n",
        "        ExVatSalesValue,\n",
        "        tf.feature_column.indicator_column(Week),\n",
        "        tf.feature_column.indicator_column(Quarter),\n",
        "        tf.feature_column.indicator_column(UKSize),\n",
        "        tf.feature_column.indicator_column(BaseColour),\n",
        "        tf.feature_column.indicator_column(ProductBrand),\n",
        "        tf.feature_column.indicator_column(Department),\n",
        "        tf.feature_column.indicator_column(ProductType),\n",
        "        tf.feature_column.indicator_column(ProductSubType),\n",
        "        tf.feature_column.indicator_column(Silo),\n",
        "        tf.feature_column.indicator_column(SubSilo),\n",
        "        tf.feature_column.indicator_column(Level),\n",
        "        tf.feature_column.indicator_column(Sport),\n",
        "        tf.feature_column.indicator_column(Supplier),\n",
        "    ]\n",
        "    \n",
        "    return feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVBjy0ezMn66",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CSV_COLUMNS = ['StockID','QuickRef','ProductBrand','Department','ProductType',\n",
        "               'ProductSubType','Silo','SubSilo','Level','BaseColour',\n",
        "               'Sport','Supplier','UKSize','UnitCostPrice','ExVatSalesValue',\n",
        "               'RRP_GBP','Quarter','Week','sumUKQuantity','sumNonUKQuantity','totalqty']\n",
        "\n",
        "DEFAULTS = [['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],\n",
        "            ['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],\n",
        "            ['UNKNOWN'],['UNKNOWN'],['UNKNOWN'],[0.0],      [0.0],      \n",
        "            [0.0],      ['UKKNOWN'],['UNKNOWN'],[0.0],      [0.0], [0.0]]\n",
        "\n",
        "\n",
        "def get_train():\n",
        "    return tf.contrib.data.make_csv_dataset(file_pattern=os.path.join(DATA_DIR, 'traindf.csv'),\n",
        "                                            batch_size=BATCH_SIZE,                                            \n",
        "                                            column_names=None,\n",
        "                                            column_defaults=DEFAULTS,\n",
        "                                            label_name=TARGET_COLUMN,\n",
        "                                            num_parallel_reads=8,\n",
        "                                            prefetch_buffer_size=1000,\n",
        "                                            sloppy=True,\n",
        "                                            num_rows_for_inference=None)\n",
        "\n",
        "\n",
        "def get_test():\n",
        "    return tf.contrib.data.make_csv_dataset(file_pattern=os.path.join(DATA_DIR, 'test_df.csv'),\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            column_names=None,\n",
        "                                            column_defaults=DEFAULTS,\n",
        "                                            label_name=TARGET_COLUMN,\n",
        "                                            shuffle=False,\n",
        "                                            num_epochs=1,\n",
        "                                            num_parallel_reads=8,\n",
        "                                            prefetch_buffer_size=1000,\n",
        "                                            sloppy=True,\n",
        "                                            num_rows_for_inference=None)\n",
        "\n",
        "def get_test_complete():\n",
        "    return tf.contrib.data.make_csv_dataset(file_pattern=os.path.join(DATA_DIR, 'test_df.csv'),\n",
        "                                            batch_size=feature_params.get('testdf_len'),\n",
        "                                            column_names=None,\n",
        "                                            column_defaults=DEFAULTS,\n",
        "                                            label_name=TARGET_COLUMN,\n",
        "                                            shuffle=False,\n",
        "                                            num_epochs=1,\n",
        "                                            num_parallel_reads=8,\n",
        "                                            prefetch_buffer_size=1000,\n",
        "                                            sloppy=True,\n",
        "                                            num_rows_for_inference=None)\n",
        "\n",
        "def get_valid_complete():\n",
        "    return tf.contrib.data.make_csv_dataset(file_pattern=os.path.join(DATA_DIR, 'valid_df.csv'),\n",
        "                                            batch_size=feature_params.get('valid_df_len'),\n",
        "                                            column_names=None,\n",
        "                                            column_defaults=DEFAULTS,\n",
        "                                            label_name=TARGET_COLUMN,\n",
        "                                            shuffle=False,\n",
        "                                            num_epochs=1,\n",
        "                                            num_parallel_reads=8,\n",
        "                                            prefetch_buffer_size=1000,\n",
        "                                            sloppy=True,\n",
        "                                            num_rows_for_inference=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnB9yM6sMn9E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_engineered(features):\n",
        "    features['UnitCostPrice'] = (features['UnitCostPrice'] - feature_params['UnitCostPrice_min']) / (feature_params['UnitCostPrice_max'] - feature_params['UnitCostPrice_min'])\n",
        "    features['RRP_GBP'] = (features['RRP_GBP'] - feature_params['RRP_GBP_min']) / (feature_params['RRP_GBP_max'] - feature_params['RRP_GBP_min'])\n",
        "    features['ExVatSalesValue'] = (features['ExVatSalesValue'] - feature_params['ExVatSalesValue_min']) / (feature_params['ExVatSalesValue_max'] - feature_params['ExVatSalesValue_min'])\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etTwmVbJMn_o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def serving_input_fn():\n",
        "    receiver_tensors  = {\n",
        "        'ProductBrand': tf.placeholder(tf.string, [None]),\n",
        "        'Department': tf.placeholder(tf.string, [None]),\n",
        "        'ProductType': tf.placeholder(tf.string, [None]),\n",
        "        'ProductSubType': tf.placeholder(tf.string, [None]),\n",
        "        'Silo': tf.placeholder(tf.string, [None]),\n",
        "        'SubSilo': tf.placeholder(tf.string, [None]),\n",
        "        'Level': tf.placeholder(tf.string, [None]),\n",
        "        'BaseColour': tf.placeholder(tf.string, [None]),\n",
        "        'Sport': tf.placeholder(tf.string, [None]),\n",
        "        'Supplier': tf.placeholder(tf.string, [None]),\n",
        "        'UKSize': tf.placeholder(tf.string, [None]),\n",
        "        'UnitCostPrice': tf.placeholder(tf.float32, [None]),\n",
        "        'ExVatSalesValue': tf.placeholder(tf.float32, [None]),\n",
        "        'RRP_GBP': tf.placeholder(tf.float32, [None]),\n",
        "        'Quarter': tf.placeholder(tf.string, [None]),\n",
        "        'Week': tf.placeholder(tf.string, [None])\n",
        "    }\n",
        "    \n",
        "    features = receiver_tensors.copy()\n",
        "    return tf.estimator.export.ServingInputReceiver(features=add_engineered(features),\n",
        "                                                    receiver_tensors=receiver_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PSZu1qdhNavX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create estimator train and evaluate function\n",
        "def train_and_evaluate(output_dir, num_train_steps):\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "    config.intra_op_parallelism_threads = 1\n",
        "    config.inter_op_parallelism_threads = 1\n",
        "    \n",
        "    my_config = tf.estimator.RunConfig(save_checkpoints_steps=np.round((feature_params['traindf_len'] / BATCH_SIZE) /2),\n",
        "                                       keep_checkpoint_max=1,\n",
        "                                       session_config=config)\n",
        "    \n",
        "    estimator = tf.estimator.DNNRegressor(hidden_units=HIDDEN_UNITS,\n",
        "                                      activation_fn=tf.nn.relu,\n",
        "                                      model_dir=output_dir,\n",
        "                                      feature_columns=create_DNNfeature_cols(),\n",
        "                                      optimizer=tf.train.AdamOptimizer(learning_rate=LEARNING_RATE),\n",
        "                                      dropout=DROPOUT,\n",
        "                                      config=my_config\n",
        "                                     )\n",
        "\n",
        "    #Add rmse evaluation metric\n",
        "    def rmse(labels, predictions):\n",
        "        pred_values = tf.cast(predictions['predictions'], tf.float64)\n",
        "        return {'rmse': tf.metrics.root_mean_squared_error(tf.cast(labels, tf.float64), pred_values)}\n",
        "    estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n",
        "    \n",
        "    \n",
        "    # Hook to stop training if loss does not decrease in over 100000 steps.\n",
        "    hook = tf.contrib.estimator.stop_if_no_decrease_hook(estimator,\n",
        "                                                         metric_name='rmse',\n",
        "                                                         max_steps_without_decrease=40000,\n",
        "                                                         eval_dir=None,\n",
        "                                                         min_steps=0,\n",
        "                                                         run_every_secs=60,\n",
        "                                                         run_every_steps=None)\n",
        "    train_spec = tf.estimator.TrainSpec(input_fn=get_train, max_steps=num_train_steps, hooks=[hook])\n",
        "    \n",
        "    exporter = tf.estimator.BestExporter('exports', serving_input_fn)\n",
        "#     exporter = tf.estimator.LatestExporter('UK', serving_input_fn)\n",
        "    \n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=get_test_complete,\n",
        "                                      steps = np.round((feature_params['traindf_len'] / BATCH_SIZE) /2),\n",
        "                                      exporters=exporter, # set the model to be exported\n",
        "                                      start_delay_secs = 0, # start evaluating after N seconds, \n",
        "                                      throttle_secs = 0  # evaluate every N seconds\n",
        "                                     )\n",
        "    \n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "    return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSm-rlMfNayO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Steps = (EPOCHS * feature_params['traindf_len']) / BATCH_SIZE\n",
        "Steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sd1w6Z3_Na1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# steps in an epoch\n",
        "(feature_params['traindf_len'] / BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IXCxOMVaNa5C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimator = train_and_evaluate(output_dir=MODEL_TYPE_DIR, num_train_steps=Steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyYkMGwKNtQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## testing"
      ]
    },
    {
      "metadata": {
        "id": "wtLent3kNa90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_evaluations = estimator.evaluate(input_fn=get_test_complete)\n",
        "# validation_evaluations = estimator.evaluate(input_fn=get_valid_complete)\n",
        "\n",
        "# print(\"Training data evaluations: {}\".format(train_evaluations))\n",
        "print(\"Test data evaluations:\\n {}\".format(test_evaluations))\n",
        "# print(\"Validation data evaluations:\\n {}\".format(validation_evaluations))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0IvZtUx2NbDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating an info file to be copied to the model zoo\n",
        "\n",
        "with open(MODEL_TYPE_DIR + '\\\\info.txt', 'w') as info_file:\n",
        "    info_file.write('\\nConfig for Initbuy')\n",
        "    info_file.write('\\nEpochs: {}'.format(EPOCHS))\n",
        "    info_file.write('\\nBatch size: {}'.format(BATCH_SIZE))\n",
        "    info_file.write('\\nHidden Units: {}'.format(HIDDEN_UNITS))\n",
        "    info_file.write('\\nLearning rate: {}'.format(LEARNING_RATE))\n",
        "    info_file.write('\\nDropout: {}'.format(DROPOUT))\n",
        "    info_file.write('\\nTF version: {}'.format(VERSION))\n",
        "    info_file.write(\"\\nTest data evaluations:\\n {}\".format(test_evaluations))\n",
        "#     info_file.write(\"\\nValidation data evaluations:\\n {}\".format(validation_evaluations))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hb_PKYyPNbA7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "killTensorboard() # needs to stop before files can be copied\n",
        "\n",
        "# making ukpreds if it doesn't yet exist\n",
        "UKpreds_ZOO_PATH = os.path.join(MODEL_ZOO, 'UKpreds')\n",
        "if not os.path.isdir(UKpreds_ZOO_PATH):\n",
        "    os.mkdir(UKpreds_ZOO_PATH)\n",
        "\n",
        "# make path for this version of the model\n",
        "UKpreds_ZOO_PATH_DT = os.path.join(UKpreds_ZOO_PATH, datetime.datetime.strftime(DATETIME, '%Y_%m_%d_%H_%M'))\n",
        "os.mkdir(UKpreds_ZOO_PATH_DT)\n",
        "\n",
        "# copy the data over\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks, ignore)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "\n",
        "copytree(MODEL_TYPE_DIR, UKpreds_ZOO_PATH_DT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q55QvfNHNa78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}