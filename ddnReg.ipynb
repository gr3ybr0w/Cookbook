{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['Sex',\n",
    "               'Length',\n",
    "               'Diameter',\n",
    "               'Height',\n",
    "               'Whole_weight',\n",
    "               'Shucked_weight',\n",
    "               'Viscera_weight',\n",
    "               'Shell_weight',\n",
    "               'Rings']\n",
    "LABEL_COLUMN = 'Rings'\n",
    "DEFAULTS = [['M'], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1]]\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 200):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        filenames_dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read lines from text files\n",
    "        textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = textlines_dataset.map(decode_csv)\n",
    "    \n",
    "        # Note:\n",
    "        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "        # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    \n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "    \n",
    "\n",
    "def get_train():\n",
    "    return read_dataset('./abalone/train_abalone.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "def get_test():\n",
    "    return read_dataset('./abalone/test_abalone.csv.csv', mode = tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_cols():\n",
    "    return [\n",
    "        tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('Sex', ['M','F', 'I'])),\n",
    "        tf.feature_column.numeric_column('Length'),\n",
    "        tf.feature_column.numeric_column('Diameter'),\n",
    "        tf.feature_column.numeric_column('Height'),\n",
    "        tf.feature_column.numeric_column('Whole_weight'),\n",
    "        tf.feature_column.numeric_column('Shucked_weight'),\n",
    "        tf.feature_column.numeric_column('Viscera_weight'),\n",
    "        tf.feature_column.numeric_column('Shell_weight'),  \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "    estimator = tf.estimator.DNNRegressor(hidden_units=[10,10],model_dir=output_dir,\n",
    "                                         feature_columns=create_feature_cols())\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train(), max_steps=num_train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_test(),\n",
    "                                     steps=None,\n",
    "                                     start_delay_secs=1,\n",
    "                                     throttle_secs=5)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './abalone_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fced0e5f160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 5 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./abalone_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 21081.371, step = 1\n",
      "INFO:tensorflow:global_step/sec: 124.99\n",
      "INFO:tensorflow:loss = 1550.5736, step = 101 (0.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.961\n",
      "INFO:tensorflow:loss = 1160.6635, step = 201 (0.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.563\n",
      "INFO:tensorflow:loss = 1142.815, step = 301 (0.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.718\n",
      "INFO:tensorflow:loss = 974.7272, step = 401 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.939\n",
      "INFO:tensorflow:loss = 1360.9043, step = 501 (0.710 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 564 into ./abalone_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 915.03577.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-16-21:29:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./abalone_trained/model.ckpt-564\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-16-21:29:22\n",
      "INFO:tensorflow:Saving dict for global step 564: average_loss = 0.0, global_step = 564, loss = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 564: ./abalone_trained/model.ckpt-564\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./abalone_trained/model.ckpt-564\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 564 into ./abalone_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 1250.685, step = 565\n",
      "INFO:tensorflow:global_step/sec: 122.24\n",
      "INFO:tensorflow:loss = 1102.8528, step = 665 (0.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.745\n",
      "INFO:tensorflow:loss = 864.23395, step = 765 (0.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.56\n",
      "INFO:tensorflow:loss = 836.09955, step = 865 (0.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.001\n",
      "INFO:tensorflow:loss = 1211.8545, step = 965 (0.781 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./abalone_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 741.5652.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-16-21:29:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./abalone_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-16-21:29:27\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.0, global_step = 1000, loss = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: ./abalone_trained/model.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "OUTDIR = './abalone_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors=True) # start fresh each time\n",
    "train_and_evaluate(OUTDIR, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
